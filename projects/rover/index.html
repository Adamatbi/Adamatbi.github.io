<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Adam Atbi</title>
        <link rel="stylesheet" href="../../style.css">
    </head>
    
    <body>
        <div class="container">
            <div class="nav-bar">
                <div class="nav-bar-contents">
                    <div class="logo">
                        <a href="../../">Adam Atbi</a>
                    </div>
                    
                    <ul class="links">
                        <li><a href="../">Projects</a></li>
                        <li><a href="../../resume.pdf">Resume</a></li>
                        <li><a href="../../about">About Me</a></ul></li>
                    </ul>
                </div>
               
            </div>
            <div class="content">
                <div class="article-page">
                    <div class="article-header">
                        <h1>Building a Robotic Car with facial recognition</h1>
                        
                    </div>
                    
                    <article class="article-content">

                        <h3 class="article-inner-title">Concept</h3>

                        <p>
                            In our embedded systems class, my team and I developed a robotic car equipped with a camera to scan for people, and navigate towards them. 
                            This project merged hardware design, embedded programming, and computer vision to create a vehicle that could drive autonomously.
                            In this blog post, I'll share the development process, challenges, and lessons learned from creating this autonomous vehicle.
                            <br><br>
                            Our stated goal was to be able to be placed on the ground and look around untill it sees a human.
                            Once finding a person it should navigate towards them correcting its path as it goes.
                            Finally when it arrives at the person it should stop at a set distance at their feet.
                            This concept could be used in a variety of applications such as a delivery or security robot.

                        </p>

                        <h3 class="article-inner-title">Physical Design</h3>

                        <p>   
                            We started with a base made from a commercial kit that included a chassis, motors, and wheels. 
                            For the computation we used a BeagleBone which is an open-source single-board computer that runs Linux.
                            To enable driving we needed to be able to control the motors. We did this using a pair of relays to control power to the wheels.
                            The BeagleBone was connected to the relays using GPIO pins. To enable the car to drive autonomously we used a camera to capture images.
                            We used a logitech camera that was connected to the BeagleBone using a USB port.
                            The camera is able to locate the directions of humans but is not able to detect exact distance so we used an ultrasonic sensor for final guidance.
                            The ultrasonic sensor was connected to the BeagleBone using GPIO. 
                        </p>
                        
                        <figure>
                            <img src="images/car.png" alt="" class="article-image">
                            <figcaption>base car kit</figcaption>
                        </figure>

                        <h3 class="article-inner-title">Software and System Architecture</h3>

                        <p>
                            The software was split into two main components, the BeagleBone ran the main program written in C and cross compiled to be ran on the BeagleBones ARM processor.
                            This program did the brunt of the work and was responsible for IO with sensors and motors, and control logic. 
                            The second component was a python script that ran on a laptop and was responsible for the facial recognition. 
                            We used AWS rekognition to detect faces and the pixel locations of faces would be sent to the BeagleBone.
                            The BeagleBone would then use this information to navigate the car. 
                            The C program was multithreaded with hardware abstraction layers for individule components to maximise modularity.
                        </p>

                        <figure>
                            <img src="images/table.jpg" alt="" class="article-image">
                            <figcaption>testing systems</figcaption>
                        </figure>

                        <p>
                            add stuff about driving
                        </p>

                        <figure>
                            <img src="images/team.jpg" alt="" class="article-image">
                            <figcaption>group photo</figcaption>
                        </figure>

                </article>
            </div>
        </div>

    </body>
</html>